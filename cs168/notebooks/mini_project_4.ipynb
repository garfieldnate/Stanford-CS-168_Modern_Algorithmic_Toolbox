{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3bc9ebb-f298-451a-82eb-6cdaea7f9d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".custom-assignment-text {\n",
       "    background-color: lightyellow;\n",
       "    border: 1px solid darkkhaki; \n",
       "    padding: 10px;\n",
       "    border-radius: 2px\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".jp-OutputArea-prompt:empty {\n",
       "  padding: 0;\n",
       "  border: 0;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Custom functionality enabled:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "* Format a code cell by entering %%black at the top of it"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "* Surround markdown cells with  `<div class=\"custom-assignment-text\">\\n\\n ... \\n\\n</div>` to format course-provided assignment text"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "* Use `ok(<message>)` to notify of a passing test"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "* Use `assert_globals_clean()` to check that all globals are managed (private, constants, etc.)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "* Use `display_table` to display data in an inline HTML table"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###########\n",
    "# PRELUDE #\n",
    "###########\n",
    "\n",
    "# auto-reload changed python files\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Format cells with %%black\n",
    "%load_ext blackcellmagic\n",
    "\n",
    "# nice interactive plots\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "# enable more math expressions in matplotlib labels\n",
    "matplotlib.rcParams['text.latex.preamble'] = r\"\\usepackage{amsmath}\"\n",
    "\n",
    "# add repository directory to include path\n",
    "from pathlib import Path\n",
    "import sys\n",
    "PROJECT_DIR = Path('../..').resolve()\n",
    "sys.path.append(str(PROJECT_DIR))\n",
    "\n",
    "import inspect\n",
    "def _acceptable_global(name, value):\n",
    "    \"\"\"Returns True if a global variable with name/value can be safely ignored\"\"\"\n",
    "    return (\n",
    "        # stuff that's normal to share everywhere\n",
    "        inspect.isroutine(value) or\n",
    "        inspect.isclass(value) or\n",
    "        inspect.ismodule(value) or\n",
    "        # leading underscore marks private variables\n",
    "        name.startswith('_') or\n",
    "        # all-caps names indicate constants\n",
    "        name.upper() == name or\n",
    "        # ignore IPython stuff\n",
    "        name in {'In', 'Out'} or \n",
    "        getattr(value, '__module__', '').startswith('IPython'))\n",
    "\n",
    "def assert_globals_clean():\n",
    "    \"\"\"Raises an assertion error if there are unmanaged global variables.\n",
    "       Variables that are considered 'managed' include those formatted with \n",
    "       ALL_CAPS (constants), _a_leading_underscore (recognized as a global but at\n",
    "       least indicated as private to the cell), classes and modules, automatic\n",
    "       imports from IPython, and functions generally.\"\"\"\n",
    "    unmanaged_globals = {k:type(v) for k, v in globals().items() if not _acceptable_global(k, v)}\n",
    "    if unmanaged_globals != {}:\n",
    "        raise AssertionError(f\"Unmanaged globals found: {unmanaged_globals}\")\n",
    "    ok(\"No unmanaged globals detected\")\n",
    "\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "def markdown(s):\n",
    "    return display(Markdown(s))\n",
    "\n",
    "def html(s):\n",
    "    return display(HTML(s))\n",
    "\n",
    "def ok(message=\"OK\"):\n",
    "    html(f\"<div class=\\\"alert alert-block alert-success\\\">{message}</div>\")\n",
    "\n",
    "html(\"\"\"\n",
    "<style>\n",
    ".custom-assignment-text {\n",
    "    background-color: lightyellow;\n",
    "    border: 1px solid darkkhaki; \n",
    "    padding: 10px;\n",
    "    border-radius: 2px\n",
    "}\n",
    "</style>\"\"\")\n",
    "\n",
    "# Fixes space left behind tqdm progress bars with leave=False\n",
    "# see https://github.com/jupyterlab/jupyterlab/issues/7354\n",
    "html(\"\"\"\n",
    "<style>\n",
    ".jp-OutputArea-prompt:empty {\n",
    "  padding: 0;\n",
    "  border: 0;\n",
    "}\n",
    "</style>\n",
    "\"\"\")\n",
    "\n",
    "def display_table(data, title, headers):\n",
    "    \"\"\"Display data in an HTML table inline in the notebook\n",
    "       data: list of lists of values to put in table rows\n",
    "       title: to set table caption\n",
    "       headers: list of table header strings\"\"\"\n",
    "    text = \"<table>\"\n",
    "    text += f\"<caption>{title}</caption>\"\n",
    "    \n",
    "    text += \"<tr>\"\n",
    "    for h in headers:\n",
    "        text += f'<th style=\"text-align:center\">{h}</th>'\n",
    "    text += \"</tr>\"\n",
    "    \n",
    "    for row in data:\n",
    "        text += \"<tr>\"\n",
    "        for value in row:\n",
    "            text += f\"<td>{value}</td>\"\n",
    "        text += \"</tr>\"\n",
    "    text += \"</table>\"\n",
    "    html(text)\n",
    "\n",
    "markdown(\"#### Custom functionality enabled:\")\n",
    "markdown(\"* Format a code cell by entering %%black at the top of it\")\n",
    "markdown(\"* Surround markdown cells with  `<div class=\\\"custom-assignment-text\\\">\\\\n\\\\n ... \\\\n\\\\n</div>` to format course-provided assignment text\")\n",
    "markdown(\"* Use `ok(<message>)` to notify of a passing test\")\n",
    "markdown(\"* Use `assert_globals_clean()` to check that all globals are managed (private, constants, etc.)\")\n",
    "markdown(\"* Use `display_table` to display data in an inline HTML table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d705ae-d780-4a49-b3b4-879803162a85",
   "metadata": {},
   "source": [
    "<div class=\"custom-assignment-text\">\n",
    "\n",
    "# Part 1: Understanding the difference between PCA and Least Squares\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1f61067-22a3-42ea-95f6-739578f80d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.testing import assert_approx_equal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48134e0-1c28-491c-a116-7e7c061b84f2",
   "metadata": {},
   "source": [
    "<div class=\"custom-assignment-text\">\n",
    "\n",
    "## Goal\n",
    "\n",
    "Both PCA and least squares regression can be viewed as algorithms for inferring (linear) relationships among data variables. In this part of the mini-project, you will develop some intuition for the differences between these two approaches, and an understanding of the settings that are better suited to using PCA or better suited to using the least squares fit.\n",
    "\n",
    "## Description\n",
    "\n",
    "The high level bit is that PCA is useful when there is a set of latent (hidden/underlying) variables, and all the coordinates of your data are linear combinations (plus noise) of those variables. The least squares fit is useful when you have direct access to the independent variables, so any noisy coordinates are linear combinations (plus noise) of known variables.\n",
    "\n",
    "We will consider a simple example with two variables, $x$ and $y$, where the true relationship between the variables is $y = 2x$. Our goal is to recover this relationship—namely, recover the coefficient “2”. In subpart (b), we consider the setting where our data consists of the actual values of $x$, and noisy estimates of $y$. In subpart (c), we consider the case where our data consists of noisy measurements of both $x$ and $y$. For each part, we will evaluate the quality of the relationship recovered by PCA, and that recovered by standard least squares regression.\n",
    "\n",
    "As a reminder, least squares regression minimizes the squared error of the dependent variable from its prediction. Namely, given $(x_i, y_i)$ pairs, least squares returns the line $l(x)$ that minimizes $\\sum_i (y_i − l(x_i))^2$.\n",
    "\n",
    "## Exercises\n",
    "\n",
    "(a) Warm-up (do not submit):\n",
    "* Write a routine `pca-recover` that takes a vector $X$ of $x_i$’s and a vector $Y$ of $y_i$’s and returns the slope of the first component of the PCA (namely, the second coordinate divided by the first).\n",
    "* Write a routine `ls-recover` that takes $X$ and $Y$ and returns the slope of the least squares fit. (Hint: since $X$ is one-dimensional, this takes a particularly simple form: $\\langle X−\\overline{X}, Y − \\overline{Y} \\rangle / || X−\\overline{X} ||_2^2$, where $\\overline{X}$ is the mean value of $X$.)\n",
    "* Set $X = [.001, .002, .003, . . . , 1]$ and $Y = 2X$. Make sure both routines return 2.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "fb93925c-1fa4-42de-b765-daa8fbf6993c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-block alert-success\">OK</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def pca(X, scale=True):\n",
    "    \"\"\"X: each column should be observations of a variable\n",
    "    scale: True if columns should be scaled to the same units\n",
    "    returns: eigenvectors of covariance matrix of X, ordered by eigenvalue descending\"\"\"\n",
    "    \n",
    "    # subtract column means so that each column is centered at the origin\n",
    "    normed = X - np.mean(X, 0)\n",
    "    # divide by standard deviation so that columns have uniform units\n",
    "    if scale:\n",
    "        normed /= np.std(normed, 0)\n",
    "        \n",
    "    cov = normed.T @ normed\n",
    "    # e.g. 1000 rows x 2 columns should yield a 2x2 matrix\n",
    "    assert cov.shape == (X.shape[1], X.shape[1])\n",
    "    \n",
    "    # `eigh` sorts the returned eigenvalues (ascending); it's also faster than plain `eig`\n",
    "    eigen_values, eigen_vectors = np.linalg.eigh(cov)\n",
    "    \n",
    "    return np.flip(eigen_vectors, axis=0)\n",
    "\n",
    "def pca_recover(X, Y):\n",
    "    mat = np.column_stack((X,Y))\n",
    "    components = pca(mat, False)\n",
    "    return components[0][1] / components[0][0]\n",
    "\n",
    "def _test_pca_recover():\n",
    "    X = np.vstack(np.arange(.001, 1.001, .001))\n",
    "    Y = 2 * X\n",
    "    assert_approx_equal(pca_recover(X, Y), 2)\n",
    "    \n",
    "_test_pca_recover()\n",
    "ok()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "19b74d75-c15b-4d32-8ca5-f76b315b72ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-block alert-success\">OK</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def ls_recover(X, Y):\n",
    "    X_centered = X - np.mean(X)\n",
    "    Y_centered = Y - np.mean(Y)\n",
    "    return np.inner(X_centered, Y_centered) / np.linalg.norm(X_centered)**2\n",
    "\n",
    "def _test_ls_recover():\n",
    "    X = np.arange(.001, 1.001, .001)\n",
    "    Y = 2 * X\n",
    "    assert_approx_equal(ls_recover(X, Y), 2)\n",
    "\n",
    "_test_ls_recover()\n",
    "ok()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd36897-a581-46b0-9c5a-09c2edc6f74f",
   "metadata": {},
   "source": [
    "(b) (4 points) Say the elements of X and Y were chosen identically and independently at random (e.g.\n",
    "every element is uniformly distributed in the square [0, 1]×[0, 1]). What would PCA recover, and what\n",
    "would LS recover?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89b2786-1d20-45c4-af11-ca100e903bdf",
   "metadata": {},
   "source": [
    "Wouldn't they both recover garbage? Or rather, PCA would sort of give a measure of how biased your uniform distribution generator is... or rather, one LS recovers the relative means and PCA recovers the relative standard deviations?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57af38ca-addd-4c32-b9dd-8e484d67c588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".custom-assignment-text {\n",
       "    background-color: lightyellow;\n",
       "    border: 1px solid darkkhaki; \n",
       "    padding: 10px;\n",
       "    border-radius: 2px\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Custom functionality enabled:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "* Format a code cell by entering %%black at the top of it"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "* Surround markdown cells with  `<div class=\"custom-assignment-text\">\\n\\n ... \\n\\n</div>` to format course-provided assignment text"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "* Use `ok(<message>)` to notify of a passing test"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "* Use `assert_globals_clean()` to check that all globals are managed (private, constants, etc.)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###########\n",
    "# PRELUDE #\n",
    "###########\n",
    "\n",
    "# auto-reload changed python files\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Format cells with %%black\n",
    "%load_ext blackcellmagic\n",
    "\n",
    "# nice interactive plots\n",
    "%matplotlib inline\n",
    "\n",
    "# add repository directory to include path\n",
    "from pathlib import Path\n",
    "import sys\n",
    "PROJECT_DIR = Path('../..').resolve()\n",
    "sys.path.append(str(PROJECT_DIR))\n",
    "\n",
    "import inspect\n",
    "def _acceptable_global(name, value):\n",
    "    \"\"\"Returns True if a global variable with name/value can be safely ignored\"\"\"\n",
    "    return (\n",
    "        # stuff that's normal to share everywhere\n",
    "        inspect.isroutine(value) or\n",
    "        inspect.isclass(value) or\n",
    "        inspect.ismodule(value) or\n",
    "        # leading underscore marks private variables\n",
    "        name.startswith('_') or\n",
    "        # all-caps names indicate constants\n",
    "        name.upper() == name or\n",
    "        # ignore IPython stuff\n",
    "        name in {'In', 'Out'} or \n",
    "        getattr(value, '__module__', '').startswith('IPython'))\n",
    "\n",
    "def assert_globals_clean():\n",
    "    \"\"\"Raises an assertion error if there are unmanaged global variables.\n",
    "       Variables that are considered 'managed' include those formatted with \n",
    "       ALL_CAPS (constants), _a_leading_underscore (recognized as a global but at\n",
    "       least indicated as private to the cell), classes and modules, automatic\n",
    "       imports from IPython, and functions generally.\"\"\"\n",
    "    unmanaged_globals = {k:type(v) for k, v in globals().items() if not _acceptable_global(k, v)}\n",
    "    if unmanaged_globals != {}:\n",
    "        raise AssertionError(f\"Unmanaged globals found: {unmanaged_globals}\")\n",
    "    ok(\"No unmanaged globals detected\")\n",
    "\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "def markdown(s):\n",
    "    return display(Markdown(s))\n",
    "\n",
    "def html(s):\n",
    "    return display(HTML(s))\n",
    "\n",
    "def ok(message=\"OK\"):\n",
    "    html(f\"<div class=\\\"alert alert-block alert-success\\\">{message}</div>\")\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    ".custom-assignment-text {\n",
    "    background-color: lightyellow;\n",
    "    border: 1px solid darkkhaki; \n",
    "    padding: 10px;\n",
    "    border-radius: 2px\n",
    "}\n",
    "</style>\"\"\"))\n",
    "\n",
    "markdown(\"#### Custom functionality enabled:\")\n",
    "markdown(\"* Format a code cell by entering %%black at the top of it\")\n",
    "markdown(\"* Surround markdown cells with  `<div class=\\\"custom-assignment-text\\\">\\\\n\\\\n ... \\\\n\\\\n</div>` to format course-provided assignment text\")\n",
    "markdown(\"* Use `ok(<message>)` to notify of a passing test\")\n",
    "markdown(\"* Use `assert_globals_clean()` to check that all globals are managed (private, constants, etc.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f81a08-4804-461b-9468-022fe798dcf8",
   "metadata": {},
   "source": [
    "<div class=\"custom-assignment-text\">\n",
    "\n",
    "## Goal of mini-project\n",
    "\n",
    "In the three problems of this mini-project, you will explore the idea of generalization, i.e., when the test error of a learned prediction function is roughly the same as its training error. You will explore how regularization and the choice of the learning algorithm (gradient descent, stochastic gradient descent, etc.) interact with generalization in a simple linear prediction setting.1 Many aspects of these relationships are still not well understood, and a fierce debate is currently raging within the Machine Learning community about whether our understanding of generalization lacks key components necessary for explaining the unreasonable effectiveness of stochastic gradient descent (particularly in the context of “deep learning”). This week will give you a glimpse of some of these mysteries.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71df7ff-f83a-4b7c-a53d-243289a8ae18",
   "metadata": {},
   "source": [
    "<div class=\"custom-assignment-text\">\n",
    "\n",
    "# Part 1: Regression, Three Ways\n",
    "\n",
    "We will consider the problem of fitting a linear model. Given $d$-dimensional input data $\\mathbf{x}^{(1)}, \\cdots, \\mathbf{x}^{(n)} ∈ ℝ^d$ with real-valued labels $y^{(1)},\\cdots, y^{(n)} ∈ ℝ$, the goal is to find the coefficient vector $\\mathbf{a}$ that minimizes the sum of the squared errors. The total squared error of $\\mathbf{a}$ can be written as $f(\\mathbf{a}) = \\sum_{i=1}^{n} f_i(\\mathbf{a})$, where $f_i(\\mathbf{a})=(\\mathbf{a}^\\top\\mathbf{x}^{(i)} - $ $y^{(i)})^2$ denotes the squared error of the $i$th data point.\n",
    "\n",
    "The data in this problem will be drawn from the following linear model. For the training data, we select $n$ data points $\\mathbf{x}^{(1)}, \\cdots, \\mathbf{x}^{(n)}$, each drawn independently from a $d$-dimensional Gaussian distribution. We then pick the \"true\" coefficient vector $\\mathbf{a}^*$ (again from a $d$-dimensional Gaussian), and give each training point $\\mathbf{x}^{(i)}$ a label equal to $(\\mathbf{a}^*)^\\top\\mathbf{x}^{(i)}$ plus some noise (which is drawn from a 1-dimensional Gaussian distribution).\n",
    "\n",
    "The following Python code will generate the data used in this problem.\n",
    "\n",
    "    d = 100 # dimensions of data\n",
    "    n = 1000 # number of data points\n",
    "    X = np.random.normal(0,1, size=(n,d))\n",
    "    a_true = np.random.normal(0,1, size=(d,1))\n",
    "    y = X.dot(a_true) + np.random.normal(0,0.5,size=(n,1))\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654cfae3-0076-401c-8998-a87f4923d282",
   "metadata": {},
   "source": [
    "<div class=\"custom-assignment-text\">\n",
    "\n",
    "(a) (4 points) Least-squares regression has the closed form solution $\\mathbf{a} = (\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top\\mathbf{y}$, which minimizes the squared error on the data. (Here $\\mathbf{X}$ is the $n×d$ data matrix as in the code above, with one row per data point, and $y$ is the $n$-vector of their labels.) Solve for $\\mathbf{a}$ and report the value of the objective function using this value $\\mathbf{a}$. For comparison, what is the total squared error if you just set $\\mathbf{a}$ to be the all 0’s vector?\n",
    "\n",
    "Comment: Computing the closed-form solution requires time $O(nd^2+d^3)$, which is slow for large $d$. Although gradient descent methods will not yield an exact solution, they do give a close approximation in much less time. For the purpose of this assignment, you can use the closed form solution as a good sanity check in the following parts.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f88e97d-e91d-49d2-9159-f87521cc7c8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

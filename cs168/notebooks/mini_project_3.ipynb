{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57af38ca-addd-4c32-b9dd-8e484d67c588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The blackcellmagic extension is already loaded. To reload it, use:\n",
      "  %reload_ext blackcellmagic\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".custom-assignment-text {\n",
       "    background-color: lightyellow;\n",
       "    border: 1px solid darkkhaki; \n",
       "    padding: 10px;\n",
       "    border-radius: 2px\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Custom functionality enabled:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "* Format a code cell by entering %%black at the top of it"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "* Surround markdown cells with  `<div class=\"custom-assignment-text\">\\n\\n ... \\n\\n</div>` to format course-provided assignment text"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "* Use `ok(<message>)` to notify of a passing test"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "* Use `assert_globals_clean()` to check that all globals are managed (private, constants, etc.)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###########\n",
    "# PRELUDE #\n",
    "###########\n",
    "\n",
    "# auto-reload changed python files\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Format cells with %%black\n",
    "%load_ext blackcellmagic\n",
    "\n",
    "# nice interactive plots\n",
    "%matplotlib inline\n",
    "\n",
    "# add repository directory to include path\n",
    "from pathlib import Path\n",
    "import sys\n",
    "PROJECT_DIR = Path('../..').resolve()\n",
    "sys.path.append(str(PROJECT_DIR))\n",
    "\n",
    "import inspect\n",
    "def _acceptable_global(name, value):\n",
    "    \"\"\"Returns True if a global variable with name/value can be safely ignored\"\"\"\n",
    "    return (\n",
    "        # stuff that's normal to share everywhere\n",
    "        inspect.isroutine(value) or\n",
    "        inspect.isclass(value) or\n",
    "        inspect.ismodule(value) or\n",
    "        # leading underscore marks private variables\n",
    "        name.startswith('_') or\n",
    "        # all-caps names indicate constants\n",
    "        name.upper() == name or\n",
    "        # ignore IPython stuff\n",
    "        name in {'In', 'Out'} or \n",
    "        getattr(value, '__module__', '').startswith('IPython'))\n",
    "\n",
    "def assert_globals_clean():\n",
    "    \"\"\"Raises an assertion error if there are unmanaged global variables.\n",
    "       Variables that are considered 'managed' include those formatted with \n",
    "       ALL_CAPS (constants), _a_leading_underscore (recognized as a global but at\n",
    "       least indicated as private to the cell), classes and modules, automatic\n",
    "       imports from IPython, and functions generally.\"\"\"\n",
    "    unmanaged_globals = {k:type(v) for k, v in globals().items() if not _acceptable_global(k, v)}\n",
    "    if unmanaged_globals != {}:\n",
    "        raise AssertionError(f\"Unmanaged globals found: {unmanaged_globals}\")\n",
    "    ok(\"No unmanaged globals detected\")\n",
    "\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "def markdown(s):\n",
    "    return display(Markdown(s))\n",
    "\n",
    "def html(s):\n",
    "    return display(HTML(s))\n",
    "\n",
    "def ok(message=\"OK\"):\n",
    "    html(f\"<div class=\\\"alert alert-block alert-success\\\">{message}</div>\")\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    ".custom-assignment-text {\n",
    "    background-color: lightyellow;\n",
    "    border: 1px solid darkkhaki; \n",
    "    padding: 10px;\n",
    "    border-radius: 2px\n",
    "}\n",
    "</style>\"\"\"))\n",
    "\n",
    "markdown(\"#### Custom functionality enabled:\")\n",
    "markdown(\"* Format a code cell by entering %%black at the top of it\")\n",
    "markdown(\"* Surround markdown cells with  `<div class=\\\"custom-assignment-text\\\">\\\\n\\\\n ... \\\\n\\\\n</div>` to format course-provided assignment text\")\n",
    "markdown(\"* Use `ok(<message>)` to notify of a passing test\")\n",
    "markdown(\"* Use `assert_globals_clean()` to check that all globals are managed (private, constants, etc.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f81a08-4804-461b-9468-022fe798dcf8",
   "metadata": {},
   "source": [
    "<div class=\"custom-assignment-text\">\n",
    "\n",
    "## Goal of mini-project\n",
    "\n",
    "In the three problems of this mini-project, you will explore the idea of generalization, i.e., when the test error of a learned prediction function is roughly the same as its training error. You will explore how regularization and the choice of the learning algorithm (gradient descent, stochastic gradient descent, etc.) interact with generalization in a simple linear prediction setting.1 Many aspects of these relationships are still not well understood, and a fierce debate is currently raging within the Machine Learning community about whether our understanding of generalization lacks key components necessary for explaining the unreasonable effectiveness of stochastic gradient descent (particularly in the context of “deep learning”). This week will give you a glimpse of some of these mysteries.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71df7ff-f83a-4b7c-a53d-243289a8ae18",
   "metadata": {},
   "source": [
    "<div class=\"custom-assignment-text\">\n",
    "\n",
    "# Part 1: Regression, Three Ways\n",
    "\n",
    "<!-- LaTeX shortcuts defined first -->$\\def\\a{{\\mathbf a}}\\def\\x{{\\mathbf x}}$We will consider the problem of fitting a linear model. Given $d$-dimensional input data $\\x^{(1)}, \\cdots, \\x^{(n)} ∈ ℝ^d$ with real-valued labels $y^{(1)},\\cdots, y^{(n)} ∈ ℝ$, the goal is to find the coefficient vector $\\a$ that minimizes the sum of the squared errors. The total squared error of $\\a$ can be written as $f(\\a) = \\sum_{i=1}^{n} f_i(\\a)$, where $f_i(\\a)=(\\a^\\top\\x^{(i)} - $ $y^{(i)})^2$ denotes the squared error of the $i$th data point.\n",
    "\n",
    "The data in this problem will be drawn from the following linear model. For the training data, we select $n$ data points $\\x^{(1)}, \\cdots, \\x^{(n)}$, each drawn independently from a $d$-dimensional Gaussian distribution. We then pick the \"true\" coefficient vector $\\a^*$ (again from a $d$-dimensional Gaussian), and give each training point $\\x^{(i)}$ a label equal to $(\\a^*)^\\top\\x^{(i)}$ plus some noise (which is drawn from a 1-dimensional Gaussian distribution).\n",
    "\n",
    "The following Python code will generate the data used in this problem.\n",
    "\n",
    "    d = 100 # dimensions of data\n",
    "    n = 1000 # number of data points\n",
    "    X = np.random.normal(0,1, size=(n,d))\n",
    "    a_true = np.random.normal(0,1, size=(d,1))\n",
    "    y = X.dot(a_true) + np.random.normal(0,0.5,size=(n,1))\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a52a8e2d-5a85-4f88-97ae-0c86c4fb8236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654cfae3-0076-401c-8998-a87f4923d282",
   "metadata": {},
   "source": [
    "<div class=\"custom-assignment-text\">\n",
    "\n",
    "<!-- LaTeX shortcuts defined first -->$\\def\\X{{\\mathbf X}}\\def\\y{{\\mathbf y}}$(a) (4 points) Least-squares regression has the closed form solution $\\a = (\\X^\\top\\X)^{-1}\\X^\\top\\y$, which minimizes the squared error on the data. (Here $\\X$ is the $n×d$ data matrix as in the code above, with one row per data point, and $y$ is the $n$-vector of their labels.) Solve for $\\a$ and report the value of the objective function using this value $\\a$. For comparison, what is the total squared error if you just set $\\a$ to be the all 0’s vector?\n",
    "\n",
    "Comment: Computing the closed-form solution requires time $O(nd^2+d^3)$, which is slow for large $d$. Although gradient descent methods will not yield an exact solution, they do give a close approximation in much less time. For the purpose of this assignment, you can use the closed form solution as a good sanity check in the following parts.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f88e97d-e91d-49d2-9159-f87521cc7c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_part1_data():\n",
    "    d = 100 # dimensions of data\n",
    "    n = 1000 # number of data points\n",
    "    X = np.random.normal(0,1, size=(n,d))\n",
    "    a_true = np.random.normal(0,1, size=(d,1))\n",
    "    y = X.dot(a_true) + np.random.normal(0,0.5,size=(n,1))\n",
    "    return X, y\n",
    "PART1_X, PART1_Y = get_part1_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4c15642-4d82-4e97-8aca-4ec2e4c6e5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_regression_closed_form(X, y):\n",
    "    a = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "    assert a.shape == (X.shape[1], 1)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c24235a-8c82-408c-aaa1-eeb1e64bc045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_error(a, X, y):\n",
    "    assert a.T.shape[1] == X.shape[1]\n",
    "    assert X.shape[0] == y.shape[0]\n",
    "    return ((a.T @ X.T - y.T)**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94bd3e02-a3ac-4c9f-abc0-27f79e8ead9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "* Error using closed-form solution: 219.03207123677566"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "* Error using all-zeros solution: 114934.12035177782"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def _estimate_and_report_errors(X, y):\n",
    "    a_estimate = least_squares_regression_closed_form(X, y)\n",
    "    a_error = squared_error(a_estimate, X, y)\n",
    "    all_zeros_error = squared_error(np.zeros((X.shape[1], 1)), X, y)\n",
    "    markdown(f\"* Error using closed-form solution: {a_error}\")\n",
    "    markdown(f\"* Error using all-zeros solution: {all_zeros_error}\")\n",
    "\n",
    "_estimate_and_report_errors(PART1_X, PART1_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdf2bec-1dbf-4e11-a5af-ea2a5eb0a550",
   "metadata": {},
   "source": [
    "<div class=\"custom-assignment-text\">\n",
    "    \n",
    "(b) (6 points) In this part, you will solve the same problem via gradient descent on the squared-error\n",
    "objective function $f(\\a) = \\sum_{i=1}^n f_i(\\a)$. Recall that the gradient of a sum of functions is the sum of their gradients. Given a point $\\a_t$, what is the gradient of $f$ at \\a_t?\n",
    "    \n",
    "Now use gradient descent to find a coefficient vector $\\a$ that approximately minimizes the least squares\n",
    "objective function over the data. Run gradient descent three times, once with each of the step sizes\n",
    "0.00005, 0.0005, and 0.0007. You should initialize $\\a$ to be the all-zero vector for all three runs. Plot the objective function value for 20 iterations for all 3 step sizes on the same graph. Comment in 3-4 sentences on how the step size can affect the convergence of gradient descent (feel free to experiment\n",
    "with other step sizes). Also report the step size that had the best final objective function value and\n",
    "the corresponding objective function value.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b601d58-4da4-4740-af75-bbc7c772ddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
